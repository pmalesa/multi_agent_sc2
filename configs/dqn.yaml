env:
  # SMACv2 environment configuration params
  distribution_config:
    n_units: 5
    n_enemies: 5
    team_gen:
      dist_type: weighted_teams
      unit_types: ["marine", "marauder", "medivac"]
      exception_unit_types: ["medivac"]
      weights: [1.0, 0.0, 0.0]
      observe: true
    start_positions:
      dist_type: surrounded_and_reflect
      p: 0.5
      n_enemies: 5
      map_x: 32
      map_y: 32
  map_name: "10gen_terran"
  debug: true
  conic_fov: false
  obs_own_pos: true
  use_unit_ranges: true
  min_attack_range: 2

agent:
  # DQN parameters
  learning_rate: 0.0005
  discount_factor: 0.99

  # TODO
  obs_dim: 64                   # Dimension of the flattened observation
  n_actions: 10                 # Number of possible discrete actions (obtained from SMACv2 env)
  learning_rate: 0.001          # Learning rate for the optimizer
  gamma: 0.99                   # Discount factor
  epsilon_start: 1.0            # Initial epsilon for epsilon-greedy policy
  epsilon_end: 0.05             # Final epsilon after decay
  epsilon_decay: 10000          # Steps over which epsilon is decayed
  buffer_size: 50000            # Max size of the replay buffer
  batch_size: 32                # Batch size for training
  target_update_interval: 100   # Number of steps after which weights are copied to the target network
  device: "cpu"                 # "cpu" or "cuda" for GPU
  # ...

training:
  episodes: 100
  # ...

evaluation:
  episodes: 100
  # ...
